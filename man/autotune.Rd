% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autotune.R
\name{autotune}
\alias{autotune}
\title{Automatic tuning}
\usage{
autotune(
  mfit,
  data,
  vars,
  target,
  hcut = 0.75,
  ignr_intr = NULL,
  pred_fun = NULL,
  lambdas = as.vector(outer(seq(1, 10, 0.1), 10^(-7:3))),
  nfolds = 5,
  strat_vars = NULL,
  glm_par = alist(),
  err_fun = mse,
  ncores = -1
)
}
\arguments{
\item{mfit}{Fitted model object (e.g., a "gbm" or "randomForest" object).}

\item{data}{Data frame containing the original training data.}

\item{vars}{Character vector specifying the features in \code{data} to use.}

\item{target}{String specifying the target (or response) variable to model.}

\item{hcut}{Numeric in the range [0,1] specifying the cut-off value for the
normalized cumulative H-statistic over all two-way interactions, ordered
from most to least important, between the features in \code{vars}. Note
that \code{hcut = 0} will consider the single most important interaction,
while \code{hcut = 1} will consider all possible two-way interactions.
Setting \code{hcut = -1} will only consider main effects in the tuning.}

\item{ignr_intr}{Optional character string specifying features to ignore when
searching for meaningful interactions to incorporate in the GLM.}

\item{pred_fun}{Optional prediction function to calculate feature effects for
the model in \code{mfit}. Requires two arguments: \code{object} and
\code{newdata}. See \code{\link[pdp:partial]{pdp::partial}} and this
\href{https://bgreenwell.github.io/pdp/articles/pdp-extending.html}{article}
for the details. See also the function \code{gbm_fun} in the example.}

\item{lambdas}{Numeric vector with the possible lambda values to explore. The
search grid is created automatically via \code{\link{lambda_grid}} such
that it contains only those values of lambda that result in a unique
grouping of the full set of features. A seperate grid is generated for main
and interaction effects, due to the scale difference in both types.}

\item{nfolds}{Integer for the number of folds in K-fold cross-validation.}

\item{strat_vars}{Character (vector) specifying the feature(s) to use for
stratified sampling. The default NULL implies no stratification is applied.}

\item{glm_par}{Named list, constructed via \code{\link{alist}}, containing
arguments to be passed on to \code{\link[stats]{glm}}. Examples are:
\code{family}, \code{weights} or \code{offset}. Note that \code{formula}
will be ignored as the GLM formula is determined by the specified
\code{target} and the automatic feature selection in the tuning process.}

\item{err_fun}{Error function to calculate the prediction errors on the
validation folds. This must be an R function which outputs a single number
and takes two vectors \code{y_pred} and \code{y_true} as input for the
predicted and true target values respectively. An additional input vector
\code{w_case} is allowed to use case weights in the error function. The
weights are determined automatically based on the \code{weights} field
supplied to \code{glm_par}. Examples already included in the package are:
\describe{ \item{mse}{mean squared error loss function (default).}
\item{wgt_mse}{weighted mean squared error loss function.}
\item{poi_dev}{Poisson deviance loss function.} } See
\code{\link{err_fun}} for details on these predefined functions.}

\item{ncores}{Integer specifying the number of cores to use. The default
\code{ncores = -1} uses all the available physical cores (not threads), as
determined by \code{parallel::detectCores(logical = 'FALSE')}.}
}
\value{
List with four elements: \describe{ \item{slct_feat}{named vector
containing the selected features (names) and the optimal number of groups
for each feature (values).} \item{best_surr}{the optimal GLM surrogate,
which is fit to all observations in \code{data}. The segmented data can be
obtained via the \code{$data} attribute of the GLM fit.} \item{tune_main}{
the cross-validation results for the main effects as a tidy data frame. The
column \code{cv_err} contains the cross-validated error, while the columns
\code{1:nfolds} contain the error on the validation folds.}
\item{tune_intr}{cross-validation results for the interaction effects.}}
}
\description{
Automated tuning process for the penalty parameter lambda, with built-in
feature selection. Lambda directly influences the granularity of the
segmentation, with low/high values resulting in a fine/coarse segmentation.
}
\examples{
\dontrun{
data('mtpl_be')
features <- setdiff(names(mtpl_be), c('id', 'nclaims', 'expo', 'long', 'lat'))
set.seed(12345)
gbm_fit <- gbm::gbm(as.formula(paste('nclaims ~',
                               paste(features, collapse = ' + '))),
                    distribution = 'poisson',
                    data = mtpl_be,
                    n.trees = 50,
                    interaction.depth = 3,
                    shrinkage = 0.1)
gbm_fun <- function(object, newdata) mean(predict(object, newdata, n.trees = object$n.trees, type = 'response'))
gbm_fit \%>\% autotune(data = mtpl_be,
                     vars = c('ageph', 'bm', 'coverage', 'fuel', 'sex', 'fleet', 'use'),
                     target = 'nclaims',
                     hcut = 0.75,
                     pred_fun = gbm_fun,
                     lambdas = as.vector(outer(seq(1, 10, 1), 10^(-6:-2))),
                     nfolds = 5,
                     strat_vars = c('nclaims', 'expo'),
                     glm_par = alist(family = poisson(link = 'log'),
                                     offset = log(expo)),
                     err_fun = poi_dev,
                     ncores = -1)
}
}
