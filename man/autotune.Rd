% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autotune.R
\name{autotune}
\alias{autotune}
\title{Automatic tuning}
\usage{
autotune(
  mfit,
  vars,
  data,
  pred_fun = NULL,
  target,
  lambdas = as.vector(outer(seq(1, 10, 0.1), 10^(-7:3))),
  nfolds = 5,
  strat_vars = NULL,
  glm_par = alist(),
  err_fun = mse,
  ncores = -1
)
}
\arguments{
\item{data}{Data frame containing the original training data.}

\item{target}{A string specifying the target (or response) variable to model.}

\item{lambdas}{A numeric vector with the possible lambda values to explore.
The search grid is created automatically via \code{\link{lambda_grid}}.
This grid contains only those values of lambda that result in a unique
grouping of the full set of features. If both main and interaction effects
are present in \code{fx_vars}, a seperate grid is generated and all the
pairwise combinations are evaluated in a cartesian grid search.}

\item{nfolds}{An integer for the number of folds in K-fold cross-validation.}

\item{strat_vars}{A character (vector) specifying the feature(s) to use for
stratified sampling. The default NULL implies no stratification is applied.}

\item{glm_par}{A named list, constructed via \code{\link{alist}}, containing
arguments to be passed on to \code{\link[stats]{glm}}. Examples are:
\code{family}, \code{weights} or \code{offset}. Note that \code{formula}
will be ignored as the GLM formula is determined by the specified
\code{target} and the automatic feature selection in the tuning process.}

\item{err_fun}{An error function to calculate the prediction errors on the
validation folds. This must be an R function which outputs a single number
and takes two vectors \code{y_pred} and \code{y_true} as input for the
predicted and true target values respectively. An additional input vector
\code{w_case} is allowed to use case weights in the error function. The
weights are determined automatically based on the \code{weights} field
supplied to \code{glm_par}. Examples already included in the package:
\describe{ \item{mse}{mean squared error loss function.}
\item{wgt_mse}{weighted mean squared error loss function.}
\item{poi_dev}{Poisson deviance loss function.} }}

\item{ncores}{An integer specifying the number of cores to use. The default
\code{ncores = -1} uses all the available physical cores (not threads), as
determined by \code{parallel::detectCores(logical = 'FALSE')}.}

\item{fx_vars}{A list of data frames containing the feature effects. These
can be obtained by applying \code{\link{insights}} on your model fit.}
}
\value{
A tidy data frame (i.e., a "tibble" object) with the cross-validation
results. The column \code{cv_err} contains the cross-validated error, while
the columns \code{1:nfolds} contain the error on the validation folds.
}
\description{
Automated tuning process for the penalty parameter lambda, with built-in
feature selection. Lambda directly influences the granularity of the
segmentation, with low/high values resulting in a fine/coarse segmentation.
}
\examples{
\dontrun{
data('mtpl_be')
features <- setdiff(names(mtpl_be),c('id', 'nclaims', 'expo'))
set.seed(12345)
gbm_fit <- gbm::gbm(as.formula(paste('nclaims ~',
                               paste(features, sep = ' ', collapse = ' + '))),
                    distribution = 'poisson',
                    data = mtpl_be,
                    n.trees = 50,
                    interaction.depth = 3,
                    shrinkage = 0.1)
gbm_fun <- function(object, newdata) mean(predict(object, newdata, n.trees = object$n.trees, type = 'response'))
gbm_fit \%>\% insights(vars = c('ageph', 'bm', 'coverage', 'fuel'),
                     data = mtpl_be,
                     interactions = 'auto',
                     hcut = 0.7,
                     pred_fun = gbm_fun) \%>\%
            autotune(data = mtpl_be,
                     target = 'nclaims',
                     lambdas = as.vector(outer(seq(1, 10, 1), 10^(-3:-1))),
                     nfolds = 5,
                     strat_vars = c('nclaims', 'expo'),
                     glm_par = alist(family = poisson(link = 'log'),
                                     offset = log(expo)),
                     err_fun = poi_dev,
                     ncores = -1)
}
}
